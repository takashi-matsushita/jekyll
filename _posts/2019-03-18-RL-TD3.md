---
layout: post
categories: [AI,]
tags: [RL,Gym,Keras]
title: Twin Delayed DDPG (TD3)
author: Takashi MATSUSHITA
---
行動空間が実数で表される環境下での強化学習、今回は [Twin Delayed DDPG (TD3)](https://arxiv.org/abs/1802.09477) を見てみる.

TD3 では学習過程の安定化の為に DDPG に以下の改良を加えている.

* 二つの (Twin) Q-value を計算し、小さい値を損失関数の計算に用いる.

  $$ y(r, s', d) = r + \gamma\min_{i=1,2}Q_{\phi_{i,target}}(s', a'(s')) $$

  $$ \mathrm{loss} = (Q_{\phi,i}(s,a) - y(r,s',d))^2 $$

* ポリシーの更新頻度を Q-function より少なくしている (delayed) 

* target action にノイズを付加している

  $$a'(s') = \textrm{clip}(\mu_{\theta_{target}}(s') +
    \textrm{clip}(\epsilon, -c, c), a\_{min}, a_{max}), \epsilon \sim \mathcal{N}(0,\sigma)$$


Pendulum-v0 を使用して、DDPG と TD3 アルゴリズムを使用した強化学習を行った.
それぞれのアルゴリズムで三回の強化学習を行い、epoch 毎に過去100回分の平均報酬を図にしてみた. 過去 20 epoch の平均報酬が -150 を超えた時点で試行を打ち切っている. 縦軸を対数にする為に報酬の符号を反転させた. 青い線が DDPG、マゼンタの線が TD3. 各試行毎のばらつきが大きいが、TD3の方が安定しているかもしれない、というところ. TD3 の収束具合は、ポリシーの更新頻度に依存するので調整が必要だが、ここではその最適化は行っていない.

<div align="center">
<svg xmlns="http://www.w3.org/2000/svg" width="300" height="250" viewBox="0 0 600 500">
  {% include figures/Pendulum.svg %}
</svg>
</div>

[Pendulum-v0](https://github.com/openai/gym/wiki/CartPole-v0) に対して double DQN/PER を試した学習結果は以下のようになった.

<div align="center">
![Pendulum-v0]({{ site.url }}/{{ site.baseurl }}/assets/img/posts/pendulum.gif){: style="max-width: 300px; height: auto;"}
</div>


今回用いたコード
  * [DDPG](https://github.com/takashi-matsushita/lab/blob/master/dnn/ddpg.py)
  * [TD3](https://github.com/takashi-matsushita/lab/blob/master/dnn/td3.py)

TD3 のノイズには DDPG と同じく、Ornstein-Uhlenbeck process を使用した.
